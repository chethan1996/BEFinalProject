{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "FacialExpressionRecognition.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chethan1996/BEFinalProject/blob/co-labs/FacialExpressionRecognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlLR78ACAuUl",
        "colab_type": "text"
      },
      "source": [
        "# Real Time Facial Expression Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHg247a-AuUo",
        "colab_type": "text"
      },
      "source": [
        "## Description\n",
        "Computer animated agents and robots bring new dimension in human computer interaction which makes it vital as how computers can affect our social life in day-to-day activities. Face to face communication is a real-time process operating at a a time scale in the order of milliseconds. The level of uncertainty at this time scale is considerable, making it necessary for humans and machines to rely on sensory rich perceptual primitives rather than slow symbolic inference processes.<br><br>\n",
        "In this project we are presenting the real time facial expression recognition of seven most basic human expressions: ANGER, DISGUST, FEAR, HAPPY, NEUTRAL SAD, SURPRISE.<br><br>\n",
        "This model can be used for prediction of expressions of both still images and real time video. However, in both the cases we have to provide image to the model. In case of real time video the image should be taken at any point in time and feed it to the model for prediction of expression. The system automatically detects face using HAAR cascade then its crops it and resize the image to a specific size and give it to the model for prediction. The model will generate seven probability values corresponding to seven expressions. The highest probability value to the corresponding expression will be the predicted expression for that image.<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9iS4z8cAuUr",
        "colab_type": "text"
      },
      "source": [
        "## Business Problem\n",
        "However, our goal here is to predict the human expressions, but we have trained our model on both human and animated images. Since, we had only approx 1500 human images which are very less to make a good model, so we took approximately 9000 animated images and leverage those animated images for training the model and ultimately do the prediction of expressions on human images.<br><br> \n",
        "For better prediction we have decided to keep the size of each image <b>350$*$350</b>.<br><br>\n",
        "<b>For any image our goal is to predict the expression of the face in that image out of seven basic human expression</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FlRW8QuAuUt",
        "colab_type": "text"
      },
      "source": [
        "## Problem Statement\n",
        "<br>\n",
        "<B>CLASSIFY THE EXPRESSION OF FACE IN IMAGE OUT OF SEVEN BASIC HUMAN EXPRESSION</B>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZFIfmQAAuUu",
        "colab_type": "text"
      },
      "source": [
        "## Performance Metric\n",
        "This is a multi-class classification problem with 7 different classes, so we have considered three performance metrics:<br>\n",
        "1. Multi-Class Log-loss\n",
        "2. Accuracy\n",
        "3. Confusion Metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-evGbX9AuUw",
        "colab_type": "text"
      },
      "source": [
        "## Source Data\n",
        "We have downloaded data from 4 different sources.<br>\n",
        "1. Human Images Source-1: http://www.consortium.ri.cmu.edu/ckagree/\n",
        "2. Human Images Source-2: http://app.visgraf.impa.br/database/faces/\n",
        "3. Human Images Source-3: http://www.kasrl.org/jaffe.html\n",
        "4. Animated Images Source: https://grail.cs.washington.edu/projects/deepexpr/ferg-db.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI-PdxO6AuUy",
        "colab_type": "text"
      },
      "source": [
        "## Real-World Business Objective & Constraints\n",
        "1. Low-latency is required.\n",
        "2. Interpretability is important for still images but not in real time. For still images, probability of predicted expressions can be given.\n",
        "3. Errors are not costly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE7mTRNsAuU0",
        "colab_type": "text"
      },
      "source": [
        "## Y- Encoded Labels\n",
        "__Angry--1__<br>\n",
        "__Disgust --2__<br>\n",
        "__Fear--3__<br>\n",
        "__Happy--4__<br>\n",
        "__Neutral--5__<br>\n",
        "__Sad--6__<br>\n",
        "__Surprise--7__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0OmH5_oAuU1",
        "colab_type": "text"
      },
      "source": [
        "## Mapping real-world to ML Problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwuFtJWoCYii",
        "colab_type": "code",
        "outputId": "6eaea8b3-8889-4f4c-9e55-4cb6996a89ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git clone https://chethan1996:Chethan%409066@github.com/chethan1996/Dataset.git"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Dataset' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14PsKukXFcfS",
        "colab_type": "code",
        "outputId": "fe1c42ff-171c-4716-f89c-0b09597f65e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BEFinalProject\tDataset  drive\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ev_qS5nbAuU3",
        "colab_type": "code",
        "outputId": "30323d53-6bfe-49d8-d771-29ec7bb61449",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import glob\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Dropout, Dense\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.applications import ResNet152V2\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "base_path=\"Dataset/Data/face-expression-recognition-dataset/images/images/\"\n",
        "print(\"All libraries imported successfully\")"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All libraries imported successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Bjb725BAuVF",
        "colab_type": "text"
      },
      "source": [
        "## 1. Reading the Data of Human Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av5uFc8uAuVH",
        "colab_type": "text"
      },
      "source": [
        "### Angry"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "QdeGI-_fAuVI",
        "colab_type": "code",
        "outputId": "e17a7241-76cb-45a0-c2dc-3f8f76c53987",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#human_angry1 = glob.glob('../Data/face-expression-recognition-dataset/images/images/train/angry')\n",
        "human_angry = glob.glob(base_path+\"train/angry/*\")\n",
        "#human_angry.remove('../Data/face-expression-recognition-dataset/images/images/train/angry\\\\Thumbs.db')\n",
        "print(\"Number of images in Angry emotion = \"+str(len(human_angry)))\n",
        "#print(human_angry)"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of images in Angry emotion = 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2mS9jY-AuVR",
        "colab_type": "code",
        "outputId": "867b70dd-4d4a-4001-e58a-2534e0981e13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "human_angry_folderName = [os.path.dirname(i)+\"/\" for i in human_angry]\n",
        "human_angry_imageName = [os.path.basename(i) for i in human_angry]\n",
        "human_angry_emotion = [[\"Angry\"]*len(human_angry)][0]\n",
        "human_angry_label = [1]*len(human_angry)\n",
        "len(human_angry_folderName), len(human_angry_imageName), len(human_angry_emotion), len(human_angry_label)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 0, 0, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4anTNt1AuVa",
        "colab_type": "code",
        "outputId": "813f3267-f419-40f9-8fac-b89ae90b96b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        }
      },
      "source": [
        "df_angry = pd.DataFrame()\n",
        "p1=df_angry[\"folderName\"] = human_angry_folderName\n",
        "p2=df_angry[\"imageName\"] = human_angry_imageName\n",
        "df_angry[\"Emotion\"] = human_angry_emotion\n",
        "df_angry[\"Labels\"] = human_angry_label\n",
        "#print(p1)\n",
        "#print(p2)\n",
        "df_angry.head()"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>folderName</th>\n",
              "      <th>imageName</th>\n",
              "      <th>Emotion</th>\n",
              "      <th>Labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [folderName, imageName, Emotion, Labels]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WN6Nj11AuV4",
        "colab_type": "text"
      },
      "source": [
        "### Disgust"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPKiQ9xRAuV6",
        "colab_type": "code",
        "outputId": "fe909710-fc9b-4c6f-ed35-b0e6664fee7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "human_disgust = glob.glob(base_path+\"train/disgust/*\")\n",
        "print(\"Number of images in Disgust emotion = \"+str(len(human_disgust)))"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of images in Disgust emotion = 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5U2dyb1MAuWE",
        "colab_type": "code",
        "outputId": "5dd16fbc-827b-4d6a-8a10-a46561ae037b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "human_disgust_folderName = [os.path.dirname(i)+\"/\" for i in human_disgust]\n",
        "human_disgust_imageName = [os.path.basename(i) for i in human_disgust]\n",
        "human_disgust_emotion = [[\"Disgust\"]*len(human_disgust)][0]\n",
        "human_disgust_label = [2]*len(human_disgust)\n",
        "\n",
        "len(human_disgust_folderName), len(human_disgust_imageName), len(human_disgust_emotion), len(human_disgust_label)"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 0, 0, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNwp2MYdAuWN",
        "colab_type": "code",
        "outputId": "408dcf59-5842-4a9a-f0ad-49db4f6ef43d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "df_disgust = pd.DataFrame()\n",
        "#print(human_disgust_folderName)\n",
        "df_disgust[\"folderName\"] = human_disgust_folderName\n",
        "df_disgust[\"imageName\"] = human_disgust_imageName\n",
        "df_disgust[\"Emotion\"] = human_disgust_emotion\n",
        "df_disgust[\"Labels\"] = human_disgust_label\n",
        "df_disgust.head()"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>folderName</th>\n",
              "      <th>imageName</th>\n",
              "      <th>Emotion</th>\n",
              "      <th>Labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [folderName, imageName, Emotion, Labels]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dw_eYwlAuWV",
        "colab_type": "text"
      },
      "source": [
        "### Fear"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGmUyk6pAuWX",
        "colab_type": "code",
        "outputId": "8206ba85-890f-4ce4-91cd-5e08419e29d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "human_fear = glob.glob(base_path+\"train/fear/*\")\n",
        "print(\"Number of images in Fear emotion = \"+str(len(human_fear)))"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of images in Fear emotion = 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGy5YQsYAuWg",
        "colab_type": "code",
        "outputId": "c8358045-ac17-492d-e1a0-9762548524df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "human_fear_folderName = [os.path.dirname(i)+\"/\" for i in human_fear]\n",
        "human_fear_imageName = [os.path.basename(i)  for i in human_fear]\n",
        "human_fear_emotion = [[\"Fear\"]*len(human_fear)][0]\n",
        "human_fear_label = [3]*len(human_fear)\n",
        "\n",
        "len(human_fear_folderName), len(human_fear_imageName), len(human_fear_emotion), len(human_fear_label)"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 0, 0, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbJS7bieAuWp",
        "colab_type": "code",
        "outputId": "64da25b2-4170-4ea3-bb04-9d0088f46fe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "df_fear = pd.DataFrame()\n",
        "df_fear[\"folderName\"] = human_fear_folderName\n",
        "df_fear[\"imageName\"] = human_fear_imageName\n",
        "df_fear[\"Emotion\"] = human_fear_emotion\n",
        "df_fear[\"Labels\"] = human_fear_label\n",
        "df_fear.head()"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>folderName</th>\n",
              "      <th>imageName</th>\n",
              "      <th>Emotion</th>\n",
              "      <th>Labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [folderName, imageName, Emotion, Labels]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXsyP04mAuWx",
        "colab_type": "text"
      },
      "source": [
        "### Happy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkZU7hKoAuWy",
        "colab_type": "code",
        "outputId": "12910c24-e795-4cb7-cd49-6e65ae064fc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "human_happy = glob.glob(base_path+'train/happy/*')\n",
        "print(\"Number of images in Happy emotion = \"+str(len(human_happy)))"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of images in Happy emotion = 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "248woUNnAuW5",
        "colab_type": "code",
        "outputId": "2a73480a-b98e-48f9-c438-725a06df431c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "human_happy_folderName = [os.path.dirname(i)+\"/\" for i in human_happy]\n",
        "human_happy_imageName = [os.path.basename(i)  for i in human_happy]\n",
        "human_happy_emotion = [[\"Happy\"]*len(human_happy)][0]\n",
        "human_happy_label = [4]*len(human_happy)\n",
        "\n",
        "len(human_happy_folderName), len(human_happy_imageName), len(human_happy_emotion), len(human_happy_label)"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 0, 0, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPP3qQ9VAuXA",
        "colab_type": "code",
        "outputId": "7a199dbb-b966-46a0-adae-139517c47578",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "df_happy = pd.DataFrame()\n",
        "df_happy[\"folderName\"] = human_happy_folderName\n",
        "df_happy[\"imageName\"] = human_happy_imageName\n",
        "df_happy[\"Emotion\"] = human_happy_emotion\n",
        "df_happy[\"Labels\"] = human_happy_label\n",
        "df_happy.head()"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>folderName</th>\n",
              "      <th>imageName</th>\n",
              "      <th>Emotion</th>\n",
              "      <th>Labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [folderName, imageName, Emotion, Labels]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1JWruhtAuXI",
        "colab_type": "text"
      },
      "source": [
        "### Neutral"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4k0PU4kAuXJ",
        "colab_type": "code",
        "outputId": "fd8445fb-6ebf-42ca-a9f2-dc5c01fac1eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "human_neutral = glob.glob(base_path+'train/neutral/*')\n",
        "print(\"Number of images in Neutral emotion = \"+str(len(human_neutral)))"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of images in Neutral emotion = 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ewjbag1nAuXQ",
        "colab_type": "code",
        "outputId": "cc4e4370-7064-4dc8-c6a6-45ea37809fd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "human_neutral_folderName = [os.path.dirname(i)+\"/\" for i in human_neutral]\n",
        "human_neutral_imageName = [os.path.basename(i)  for i in human_neutral]\n",
        "human_neutral_emotion = [[\"Neutral\"]*len(human_neutral)][0]\n",
        "human_neutral_label = [5]*len(human_neutral)\n",
        "\n",
        "len(human_neutral_folderName), len(human_neutral_imageName), len(human_neutral_emotion), len(human_neutral_label)"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 0, 0, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpMP8M2CAuXZ",
        "colab_type": "code",
        "outputId": "4fd9630c-dcd8-4244-d144-88fd3fe452d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        }
      },
      "source": [
        "df_neutral = pd.DataFrame()\n",
        "df_neutral[\"folderName\"] = human_neutral_folderName\n",
        "df_neutral[\"imageName\"] = human_neutral_imageName\n",
        "df_neutral[\"Emotion\"] = human_neutral_emotion\n",
        "df_neutral[\"Labels\"] = human_neutral_label\n",
        "df_neutral.head()"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>folderName</th>\n",
              "      <th>imageName</th>\n",
              "      <th>Emotion</th>\n",
              "      <th>Labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [folderName, imageName, Emotion, Labels]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mwBoM-0AuXi",
        "colab_type": "text"
      },
      "source": [
        "### Sad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je53cFoyAuXk",
        "colab_type": "code",
        "outputId": "81c5477f-23fd-4dee-9cfc-cd82b097b6a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "human_sad = glob.glob(base_path+'train/sad/*')\n",
        "print(\"Number of images in Sad emotion = \"+str(len(human_sad)))"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of images in Sad emotion = 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-V5d0UiAuXr",
        "colab_type": "code",
        "outputId": "07170be5-1500-455d-92f1-c2aafd21c800",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "human_sad_folderName = [os.path.dirname(i)+\"/\" for i in human_sad]\n",
        "human_sad_imageName = [os.path.basename(i)  for i in human_sad]\n",
        "human_sad_emotion = [[\"Sad\"]*len(human_sad)][0]\n",
        "human_sad_label = [6]*len(human_sad)\n",
        "\n",
        "len(human_sad_folderName), len(human_sad_imageName), len(human_sad_emotion), len(human_sad_label)"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 0, 0, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XS-F6nbEAuXx",
        "colab_type": "code",
        "outputId": "0a7cabf2-8dea-4120-bf75-fa2b72321437",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "df_sad = pd.DataFrame()\n",
        "df_sad[\"folderName\"] = human_sad_folderName\n",
        "df_sad[\"imageName\"] = human_sad_imageName\n",
        "df_sad[\"Emotion\"] = human_sad_emotion\n",
        "df_sad[\"Labels\"] = human_sad_label\n",
        "df_sad.head()"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>folderName</th>\n",
              "      <th>imageName</th>\n",
              "      <th>Emotion</th>\n",
              "      <th>Labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [folderName, imageName, Emotion, Labels]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0md5KUPWAuX4",
        "colab_type": "text"
      },
      "source": [
        "### Surprise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwNSfm_mAuX6",
        "colab_type": "code",
        "outputId": "cb745c26-0d39-4d5a-b3fe-f905701ab94f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "human_surprise = glob.glob(base_path+'train/surprise/*')\n",
        "#human_surprise.remove('../Data/Human/Surprise\\\\Thumbs.db')\n",
        "print(\"Number of images in Surprise emotion = \"+str(len(human_surprise)))"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of images in Surprise emotion = 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APb8WSr0AuYC",
        "colab_type": "code",
        "outputId": "bda2fdb9-f0dc-4057-c905-9a6eb68ab6d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "human_surprise_folderName = [os.path.dirname(i) +\"/\" for i in human_surprise]\n",
        "human_surprise_imageName = [os.path.basename(i)  for i in human_surprise]\n",
        "human_surprise_emotion = [[\"Surprise\"]*len(human_surprise)][0]\n",
        "human_surprise_label = [7]*len(human_surprise)\n",
        "\n",
        "len(human_surprise_folderName), len(human_surprise_imageName), len(human_surprise_emotion), len(human_surprise_label)"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 0, 0, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqr-jwB2AuYJ",
        "colab_type": "code",
        "outputId": "ca70adc6-bcd7-4f6d-b1ef-19e6a9b3e286",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "df_surprise = pd.DataFrame()\n",
        "df_surprise[\"folderName\"] = human_surprise_folderName\n",
        "df_surprise[\"imageName\"] = human_surprise_imageName\n",
        "df_surprise[\"Emotion\"] = human_surprise_emotion\n",
        "df_surprise[\"Labels\"] = human_surprise_label\n",
        "df_surprise.head()"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>folderName</th>\n",
              "      <th>imageName</th>\n",
              "      <th>Emotion</th>\n",
              "      <th>Labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [folderName, imageName, Emotion, Labels]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJPt5AnBAuYR",
        "colab_type": "code",
        "outputId": "7d4e5f12-0bbe-4448-8e20-89dbc6f3b158",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "length = df_angry.shape[0] + df_disgust.shape[0] + df_fear.shape[0] + df_happy.shape[0] + df_neutral.shape[0] + df_sad.shape[0] + df_surprise.shape[0]\n",
        "print(\"Total number of images in all the emotions = \"+str(length))"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of images in all the emotions = 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8iYsTqFAuYZ",
        "colab_type": "text"
      },
      "source": [
        "### Concatenating all dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yP94D7aaAuYa",
        "colab_type": "code",
        "outputId": "90cb88b5-7a51-4376-8f8c-13ea4b3a53ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "frames = [df_angry, df_disgust, df_fear, df_happy, df_neutral, df_sad, df_surprise]\n",
        "Final_human = pd.concat(frames)\n",
        "Final_human.shape"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSixjYDDAuYh",
        "colab_type": "code",
        "outputId": "d429e1a7-fbdd-4a92-8c07-66c45c5911a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "Final_human.reset_index(inplace = True, drop = True)\n",
        "Final_human = Final_human.sample(frac = 1.0)   #shuffling the dataframe\n",
        "Final_human.reset_index(inplace = True, drop = True)\n",
        "Final_human.head()"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>folderName</th>\n",
              "      <th>imageName</th>\n",
              "      <th>Emotion</th>\n",
              "      <th>Labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [folderName, imageName, Emotion, Labels]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyHUwg8IAuYo",
        "colab_type": "text"
      },
      "source": [
        "## 2. Train, CV and Test Split for Human Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFAX_GBOAuYq",
        "colab_type": "code",
        "outputId": "517bf377-fa9b-440a-9a2d-ce7f531c22ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "source": [
        "df_human_train_data, df_human_test = train_test_split(Final_human, stratify=Final_human[\"Labels\"], test_size = 0.197860)\n",
        "df_human_train, df_human_cv = train_test_split(df_human_train_data, stratify=df_human_train_data[\"Labels\"], test_size = 0.166666)\n",
        "df_human_train.shape, df_human_cv.shape, df_human_test.shape "
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-167-7f943dcaa737>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_human_train_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_human_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFinal_human\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFinal_human\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.197860\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_human_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_human_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_human_train_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_human_train_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.166666\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_human_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_human_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_human_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2120\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2121\u001b[0m     n_train, n_test = _validate_shuffle_split(n_samples, test_size, train_size,\n\u001b[0;32m-> 2122\u001b[0;31m                                               default_test_size=0.25)\n\u001b[0m\u001b[1;32m   2123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2124\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   1803\u001b[0m             \u001b[0;34m'resulting train set will be empty. Adjust any of the '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m             'aforementioned parameters.'.format(n_samples, test_size,\n\u001b[0;32m-> 1805\u001b[0;31m                                                 train_size)\n\u001b[0m\u001b[1;32m   1806\u001b[0m         )\n\u001b[1;32m   1807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.19786 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FihmjD5pAuYx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_human_train.reset_index(inplace = True, drop = True)\n",
        "df_human_train.to_pickle(base_path+\"train.pkl\")\n",
        "\n",
        "df_human_test.reset_index(inplace = True, drop = True)\n",
        "df_human_test.to_pickle(base_path+\"validation.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FKZOOJWAuY5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_human_train = pd.read_pickle(base_path+\"train.pkl\")\n",
        "df_human_train.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUialLziAuY_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_human_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "739aTsRQAuZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_human_test = pd.read_pickle(base_path+\"validation.pkl\")\n",
        "df_human_test.head()\n",
        "#print(df_human_test[\"folderName\"][1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxF0i0jkAuZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_human_test.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9vGlU4gAuZq",
        "colab_type": "text"
      },
      "source": [
        "## 3. Analysing Data of Human Images\n",
        "### Distribution of class labels in Train and Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwdHJm1UAuZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_temp_train = df_human_train.sort_values(by = \"Labels\", inplace = False)\n",
        "df_temp_test = df_human_test.sort_values(by = \"Labels\", inplace = False)\n",
        "\n",
        "TrainData_distribution = df_human_train[\"Emotion\"].value_counts().sort_index()\n",
        "TestData_distribution = df_human_test[\"Emotion\"].value_counts().sort_index()\n",
        "\n",
        "TrainData_distribution_sorted = sorted(TrainData_distribution.items(), key = lambda d: d[1], reverse = True)\n",
        "TestData_distribution_sorted = sorted(TestData_distribution.items(), key = lambda d: d[1], reverse = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "cWOr_e3gAuZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize = (10, 6))\n",
        "ax = fig.add_axes([0,0,1,1])\n",
        "ax.set_title(\"Count of each Emotion in Train Data\", fontsize = 20)\n",
        "sns.countplot(x = \"Emotion\", data = df_temp_train)\n",
        "plt.grid()\n",
        "for i in ax.patches:\n",
        "    ax.text(x = i.get_x() + 0.2, y = i.get_height()+1.5, s = str(i.get_height()), fontsize = 20, color = \"grey\")\n",
        "plt.xlabel(\"\")\n",
        "plt.ylabel(\"Count\", fontsize = 15)\n",
        "plt.tick_params(labelsize = 15)\n",
        "plt.xticks(rotation = 40)\n",
        "plt.show()\n",
        "\n",
        "for i in TrainData_distribution_sorted:\n",
        "    print(\"Number of training data points in class \"+str(i[0])+\" = \"+str(i[1])+ \"(\"+str(np.round(((i[1]/df_temp_train.shape[0])*100), 4))+\"%)\")\n",
        "\n",
        "print(\"-\"*80)\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize = (10, 6))\n",
        "ax = fig.add_axes([0,0,1,1])\n",
        "ax.set_title(\"Count of each Emotion in Test Data\", fontsize = 20)\n",
        "sns.countplot(x = \"Emotion\", data = df_temp_test)\n",
        "plt.grid()\n",
        "for i in ax.patches:\n",
        "    ax.text(x = i.get_x() + 0.27, y = i.get_height()+0.2, s = str(i.get_height()), fontsize = 20, color = \"grey\")\n",
        "plt.xlabel(\"\")\n",
        "plt.ylabel(\"Count\", fontsize = 15)\n",
        "plt.tick_params(labelsize = 15)\n",
        "plt.xticks(rotation = 40)\n",
        "plt.show()\n",
        "\n",
        "for i in TestData_distribution_sorted:\n",
        "    print(\"Number of training data points in class \"+str(i[0])+\" = \"+str(i[1])+ \"(\"+str(np.round(((i[1]/df_temp_test.shape[0])*100), 4))+\"%)\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_urdjhQAuZ8",
        "colab_type": "text"
      },
      "source": [
        "## 4. Pre-Processing Human Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W939F6YnAuZ9",
        "colab_type": "text"
      },
      "source": [
        "### 4.1 Converting all the images to grayscale and save them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnVlJzexAuZ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convt_to_gray(df):\n",
        "    count = 0\n",
        "    for i in range(len(df)):\n",
        "        path1 = df[\"folderName\"][i]\n",
        "        path2 = df[\"imageName\"][i]\n",
        "        #print(os.path.join(path1, path2))\n",
        "        img = cv2.imread(os.path.join(path1, path2))\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        cv2.imwrite(os.path.join(path1, path2), gray)\n",
        "        count += 1\n",
        "    print(\"Total number of images converted and saved = \"+str(count))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHISqvvFAuaF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "convt_to_gray(df_human_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtS2QxR1AuaT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "convt_to_gray(df_human_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeE1VUtTAuad",
        "colab_type": "text"
      },
      "source": [
        "### 4.2 Detecting face in image using HAAR then crop it then resize then save the image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6xhlo4eK1pI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/chethan1996/BEFinalProject.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHInTe3eAuaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#detect the face in image using HAAR cascade then crop it then resize it and finally save it.\n",
        "face_cascade = cv2.CascadeClassifier('BEFinalProject/haarcascade_frontalface_default.xml') \n",
        "#download this xml file from link: https://github.com/opencv/opencv/tree/master/data/haarcascades.\n",
        "def face_det_crop_resize(img_path):\n",
        "    img = cv2.imread(img_path)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "    \n",
        "    for (x,y,w,h) in faces:\n",
        "        face_clip = img[y:y+h, x:x+w]  #cropping the face in image\n",
        "        cv2.imwrite(img_path, cv2.resize(face_clip, (48, 48)))  #resizing image then saving it"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XzjOr-iAuam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, d in df_human_train.iterrows():\n",
        "    img_path = os.path.join(d[\"folderName\"], d[\"imageName\"])\n",
        "    face_det_crop_resize(img_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQ-1nUoVAuau",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, d in df_human_cv.iterrows():\n",
        "    img_path = os.path.join(d[\"folderName\"], d[\"imageName\"])\n",
        "    face_det_crop_resize(img_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xWfgEagAua2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, d in df_human_test.iterrows():\n",
        "    img_path = os.path.join(d[\"folderName\"], d[\"imageName\"])\n",
        "    face_det_crop_resize(img_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CqvhOMdAua8",
        "colab_type": "text"
      },
      "source": [
        "## 5. Creating bottleneck features from VGG-16 model. Here, we are using Transfer learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cxnzg8w1Aua9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Train_Data = pd.read_pickle(base_path+\"train.pkl\")\n",
        "Test_Data = pd.read_pickle(base_path+\"validation.pkl\")\n",
        "Train_Data.shape, Test_Data.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwjJ_3TrAubD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Train_Data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18adzuGcAubJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTnu20hJAubO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TrainData_batch_pointer = 0\n",
        "TestData_batch_pointer = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RYv6LHFAubZ",
        "colab_type": "text"
      },
      "source": [
        "### 5.1 Bottleneck features for Train Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "QCD1UgCCAuba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TrainData_Labels = pd.get_dummies(Train_Data[\"Labels\"]).as_matrix()\n",
        "TrainData_Labels.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcyyMxqrAubk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trdata = ImageDataGenerator()\n",
        "traindata = trdata.flow_from_directory(directory=base_path+\"/train\",target_size=(48,48))\n",
        "tsdata = ImageDataGenerator()\n",
        "testTestdata = tsdata.flow_from_directory(directory=base_path+\"/validation\", target_size=(48,48))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUJpD4BnAubw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Train_Data = Train_Data.dropna(how='any',axis=0)\n",
        "print(int(len(Train_Data)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9naWhVbms3P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_images = []\n",
        "batch_labels = []\n",
        "for i in range(len(Test_Data)):\n",
        "    #print(i)\n",
        "    path1 = Test_Data.iloc[i][\"folderName\"]\n",
        "    path2 = Test_Data.iloc[ i][\"imageName\"]\n",
        "    read_image = cv2.imread(os.path.join(path1, path2))\n",
        "    #read_image_final = read_image/255.0  #here, we are normalizing the images\n",
        "    try:\n",
        "        if(read_image.shape!=(48,48,3)):\n",
        "            print(\"Removed {}, {}\".format(os.path.join(path1, path2), read_image.shape))\n",
        "            os.remove(os.path.join(path1, path2))\n",
        "        else:\n",
        "            print(read_image.shape)\n",
        "    except AttributeError :\n",
        "        print(read_image)\n",
        "        print(\"gotti\")\n",
        "        os.remove(os.path.join(path1, path2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fkIy1S2xgqQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TestData_batch_pointer=0\n",
        "print(TrainData_batch_pointer)\n",
        "print(TestData_batch_pointer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6PcfVIGAub4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loadTrainBatch(batch_size):\n",
        "    global TrainData_batch_pointer\n",
        "    batch_images = []\n",
        "    batch_labels = []\n",
        "    for i in range(batch_size):\n",
        "        path1 = Train_Data.iloc[TrainData_batch_pointer + i][\"folderName\"]\n",
        "        path2 = Train_Data.iloc[TrainData_batch_pointer + i][\"imageName\"]\n",
        "        read_image = cv2.imread(os.path.join(path1, path2))\n",
        "        #read_image_final = read_image/255.0  #here, we are normalizing the images\n",
        "        #print(read_image_final)\n",
        "        batch_images.append(read_image)\n",
        "        try:\n",
        "            print(read_image.shape)\n",
        "        except AttributeError :\n",
        "            print(\"gotti\")\n",
        "            print(read_image)\n",
        "            print(os.path.join(path1, path2))\n",
        "            os.remove(os.path.join(path1, path2))\n",
        "        batch_labels.append(TrainData_Labels[TrainData_batch_pointer + i]) #appending corresponding labels\n",
        "        \n",
        "    TrainData_batch_pointer += batch_size\n",
        "        \n",
        "    return np.array(batch_images), np.array(batch_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "NTpv0nMpAucC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating bottleneck features for train data using VGG-16- Image-net model\n",
        "img_height,img_width=48,48\n",
        "model=ResNet152V2(weights= 'imagenet', include_top=False,input_shape= (img_height,img_width,3))\n",
        "#model = VGG16(weights='imagenet', include_top=False)\n",
        "SAVEDIR = \"Dataset/Data/Bottleneck_Features/Bottleneck_TrainData/\"\n",
        "SAVEDIR_LABELS = \"Dataset/Data/Bottleneck_Features/TrainData_Labels/\"\n",
        "batch_size =10\n",
        "for i in range(int(len(Train_Data)/batch_size)):\n",
        "    x, y = loadTrainBatch(batch_size)\n",
        "    print(\"Batch {} loaded\".format(i+1))\n",
        "    \n",
        "    np.save(os.path.join(SAVEDIR_LABELS, \"bottleneck_labels_{}\".format(i+1)), y)\n",
        "    \n",
        "    print(\"Creating bottleneck features for batch {}\". format(i+1))\n",
        "    bottleneck_features = model.predict(x)\n",
        "    \n",
        "    np.save(os.path.join(SAVEDIR, \"bottleneck_{}\".format(i+1)), bottleneck_features)\n",
        "    print(\"Bottleneck features for batch {} created and saved\\n\".format(i+1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFCLpTzJAucI",
        "colab_type": "text"
      },
      "source": [
        "## Bottle neck feature for Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QvPh8daAucJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TestData_Labels = pd.get_dummies(Test_Data[\"Labels\"]).as_matrix()\n",
        "TestData_Labels.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAY4_-gXAucV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loadTestDataBatch(batch_size):\n",
        "    global TestData_batch_pointer\n",
        "    batch_images = []\n",
        "    batch_labels = []\n",
        "    for i in range(batch_size):\n",
        "        path1 = Test_Data.iloc[TestData_batch_pointer + i][\"folderName\"]\n",
        "        path2 = Test_Data.iloc[TestData_batch_pointer + i][\"imageName\"]\n",
        "        read_image = cv2.imread(os.path.join(path1, path2))\n",
        "        read_image_final = read_image/255.0  #here, we are normalizing the images\n",
        "        batch_images.append(read_image_final)\n",
        "        \n",
        "        batch_labels.append(TestData_Labels[TestData_batch_pointer + i]) #appending corresponding labels\n",
        "        \n",
        "    TestData_batch_pointer += batch_size\n",
        "        \n",
        "    return np.array(batch_images), np.array(batch_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "O4Srvl9MAucf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating bottleneck features for Test Humans data using VGG-16- Image-net model\n",
        "img_height,img_width=48,48\n",
        "model = ResNet152V2(weights='imagenet', include_top=False,input_shape= (img_height,img_width,3))\n",
        "SAVEDIR = \"Dataset/Data/Bottleneck_Features/Bottleneck_TestData/\"\n",
        "SAVEDIR_LABELS = \"Dataset/Data/Bottleneck_Features/TestData_Labels/\"\n",
        "batch_size = 10\n",
        "for i in range(int(len(Test_Data)/batch_size)):\n",
        "    x, y = loadTestDataBatch(batch_size)\n",
        "    print(\"Batch {} loaded\".format(i+1))\n",
        "    \n",
        "    np.save(os.path.join(SAVEDIR_LABELS, \"bottleneck_labels_{}\".format(i+1)), y)\n",
        "    \n",
        "    print(\"Creating bottleneck features for batch {}\". format(i+1))\n",
        "    bottleneck_features = model.predict(x)\n",
        "    np.save(os.path.join(SAVEDIR, \"bottleneck_{}\".format(i+1)), bottleneck_features)\n",
        "    print(\"Bottleneck features for batch {} created and saved\\n\".format(i+1))\n",
        "\n",
        "leftover_points = len(Test_Data) - TestData_batch_pointer\n",
        "x, y = loadTestDataBatch(leftover_points)\n",
        "np.save(os.path.join(SAVEDIR_LABELS, \"bottleneck_labels_{}\".format(int(len(Test_Data)/batch_size) + 1)), y)\n",
        "bottleneck_features = model.predict(x)\n",
        "np.save(os.path.join(SAVEDIR, \"bottleneck_{}\".format(int(len(Test_Data)/batch_size) + 1)), bottleneck_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubQKph6PAucl",
        "colab_type": "text"
      },
      "source": [
        "## 11. Modelling & Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IycgHIxAucm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "no_of_classes = 7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WORBJGWAucs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validation(model):\n",
        "    if (model is None):\n",
        "      print(\"No model passed for validation, loading model from Dataset/Data/Model_Save/model.h5\")\n",
        "      model = load_model(\"Dataset/Data/Model_Save/model.h5\")\n",
        "    predicted_labels = []\n",
        "    true_labels = []\n",
        "    batch_size = 10\n",
        "    total_files = int(len(Test_Data) / batch_size) #here, I have added 2 because there are 30 files in Test_Humans\n",
        "    for i in range(1, total_files, 1):\n",
        "        img_load = np.load(\"Dataset/Data/Bottleneck_Features/Bottleneck_TestData/bottleneck_{}.npy\".format(i))\n",
        "        img_label = np.load(\"Dataset/Data/Bottleneck_Features/TestData_Labels/bottleneck_labels_{}.npy\".format(i))\n",
        "        img_bundle = img_load.reshape(img_load.shape[0], img_load.shape[1]*img_load.shape[2]*img_load.shape[3])\n",
        "        for j in range(img_bundle.shape[0]):\n",
        "            img = img_bundle[j]\n",
        "            img = img.reshape(1, img_bundle.shape[1])\n",
        "            pred = model.predict(img)\n",
        "            predicted_labels.append(pred[0].argmax())\n",
        "            true_labels.append(img_label[j].argmax())\n",
        "    acc = accuracy_score(true_labels, predicted_labels)\n",
        "    print(\"Accuracy on Test Data = {}%\".format(np.round(float(acc*100), 2)))\n",
        "    return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "pSGpSFHoAucz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#training the model\n",
        "SAVEDIR_TRAIN = \"Dataset/Data/Bottleneck_Features/Bottleneck_TrainData/\"\n",
        "SAVEDIR_TRAIN_LABELS = \"Dataset/Data/Bottleneck_Features/TrainData_Labels/\"\n",
        "\n",
        "SAVER = \"Dataset/Data/Model_Save/\"\n",
        "\n",
        "#input_shape =48*48*3   #this is the shape of bottleneck feature of each image which comes after passing the image through VGG-16\n",
        "\n",
        "#model = model(input_shape)\n",
        "#model.load_weights(os.path.join(SAVER, \"model.h5\"))\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(512, input_shape=(8192,), activation='relu'))\n",
        "model.add(Dense(512, input_shape=(256,), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(128, input_shape=(256,), activation='relu'))\n",
        "model.add(Dense(output_dim = no_of_classes, activation='softmax')) \n",
        "\n",
        "model.summary()\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics = [\"accuracy\"])\n",
        "\n",
        "epochs = 200\n",
        "batch_size = 50\n",
        "step = 0\n",
        "Train_bottleneck_files = int(len(Train_Data) / batch_size)\n",
        "epoch_number, Train_loss, Train_acc ,test_acc = [],[], [], []\n",
        "model_save_interval = 3 # every x epochs for saving model\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    avg_epoch_Tr_loss, avg_epoch_Tr_acc= 0, 0\n",
        "    epoch_number.append(epoch + 1)\n",
        "    \n",
        "    for i in range(Train_bottleneck_files):\n",
        "        \n",
        "        step += 1\n",
        "        \n",
        "        #loading batch of train bottleneck features for training MLP.\n",
        "        X_Train_load = np.load(os.path.join(SAVEDIR_TRAIN, \"bottleneck_{}.npy\".format(i+1)))\n",
        "        X_Train = X_Train_load.reshape(X_Train_load.shape[0], X_Train_load.shape[1]*X_Train_load.shape[2]*X_Train_load.shape[3])\n",
        "        Y_Train = np.load(os.path.join(SAVEDIR_TRAIN_LABELS, \"bottleneck_labels_{}.npy\".format(i+1)))\n",
        "                \n",
        "        Train_Loss, Train_Accuracy = model.train_on_batch(X_Train, Y_Train) #train the model on batch\n",
        "        \n",
        "        # print(\"Epoch: {}, Step: {}, Tr_Loss: {}, Tr_Acc: {}\".format(epoch+1, step, np.round(float(Train_Loss), 2), np.round(float(Train_Accuracy), 2)))\n",
        "        avg_epoch_Tr_loss += Train_Loss / Train_bottleneck_files\n",
        "        avg_epoch_Tr_acc += Train_Accuracy / Train_bottleneck_files\n",
        "        \n",
        "    print(\"Avg_Train_Loss: {}, Avg_Train_Acc: {}\".format(np.round(float(avg_epoch_Tr_loss), 2), np.round(float(avg_epoch_Tr_acc), 2)))\n",
        "\n",
        "    Train_loss.append(avg_epoch_Tr_loss)\n",
        "    Train_acc.append(avg_epoch_Tr_acc)\n",
        "    \n",
        "    #if(epoch < 5 or epoch % model_save_interval == 0 or epoch == epochs )\n",
        "    if(epoch == epochs ):\n",
        "      model.save(os.path.join(SAVER, \"model.h5\"))  #saving the model for last epoch\n",
        "      model.save_weights(os.path.join(SAVER, \"model_weights.h5\")) #saving the weights of model on each epoch\n",
        "      print(\"Model and weights saved at epoch {}\".format(epoch + 1))\n",
        "    test_acc_val = validation(model)\n",
        "    test_acc.append(test_acc_val)\n",
        "    print(\"Test Accuracy: \",test_acc_val)\n",
        "      ## test validation accuracy\n",
        "          \n",
        "log_frame = pd.DataFrame(columns = [\"Epoch\", \"Train_Loss\", \"Train_Accuracy\",\"Test_Accuracy\"])\n",
        "log_frame[\"Epoch\"] = epoch_number\n",
        "log_frame[\"Train_Loss\"] = Train_loss\n",
        "log_frame[\"Train_Accuracy\"] = Train_acc\n",
        "log_frame[\"Test_Accuracy\"]=test_acc\n",
        "log_frame.to_csv(\"Dataset/Data/Logs/Log.csv\", index = False)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BOfBAQgAueH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "log = pd.read_csv(\"Dataset/Data/Logs/Log.csv\")\n",
        "log"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxJHhoWyAueM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plotting(epoch, train_acc, val,title):\n",
        "    fig, axes = plt.subplots(1,1, figsize = (12, 8))\n",
        "    axes.plot(epoch, train_acc, color = 'red', label = \"Train_Accuracy\")\n",
        "    axes.plot(epoch, val, color = 'blue', label = \"Validation_Accuracy\")\n",
        "    axes.set_title(title, fontsize = 25)\n",
        "    axes.set_xlabel(\"Epochs\", fontsize = 20)\n",
        "    axes.set_ylabel(\"Accuracy\", fontsize = 20)\n",
        "    axes.grid()\n",
        "    axes.legend(fontsize = 20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7N7aR-aAueQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plotting(list(log[\"Epoch\"]), list(log[\"Train_Accuracy\"]),list(log[\"Test_Accuracy\"]),\"EPOCH VS ACCURACY\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "By0ndmBKAueW",
        "colab_type": "text"
      },
      "source": [
        "## 12. Checking Test Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ba1SFHZFAueX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_confusionMatrix(Y_TestLabels, PredictedLabels):\n",
        "    confusionMatx = confusion_matrix(Y_TestLabels, PredictedLabels)\n",
        "    \n",
        "    precision = confusionMatx/confusionMatx.sum(axis = 0)\n",
        "    \n",
        "    recall = (confusionMatx.T/confusionMatx.sum(axis = 1)).T\n",
        "    \n",
        "    sns.set(font_scale=1.5)\n",
        "    \n",
        "    # confusionMatx = [[1, 2],\n",
        "    #                  [3, 4]]\n",
        "    # confusionMatx.T = [[1, 3],\n",
        "    #                   [2, 4]]\n",
        "    # confusionMatx.sum(axis = 1)  axis=0 corresponds to columns and axis=1 corresponds to rows in two diamensional array\n",
        "    # confusionMatx.sum(axix =1) = [[3, 7]]\n",
        "    # (confusionMatx.T)/(confusionMatx.sum(axis=1)) = [[1/3, 3/7]\n",
        "    #                                                  [2/3, 4/7]]\n",
        "\n",
        "    # (confusionMatx.T)/(confusionMatx.sum(axis=1)).T = [[1/3, 2/3]\n",
        "    #                                                    [3/7, 4/7]]\n",
        "    # sum of row elements = 1\n",
        "    \n",
        "    labels = [\"ANGRY\", \"DISGUST\", \"FEAR\", \"HAPPY\", \"NEUTRAL\", \"SAD\", \"SURPRISE\"]\n",
        "    \n",
        "    plt.figure(figsize=(16,7))\n",
        "    sns.heatmap(confusionMatx, cmap = \"Blues\", annot = True, fmt = \".1f\", xticklabels=labels, yticklabels=labels)\n",
        "    plt.title(\"Confusion Matrix\", fontsize = 30)\n",
        "    plt.xlabel('Predicted Class', fontsize = 20)\n",
        "    plt.ylabel('Original Class', fontsize = 20)\n",
        "    plt.tick_params(labelsize = 15)\n",
        "    plt.xticks(rotation = 90)\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"-\"*125)\n",
        "    \n",
        "    plt.figure(figsize=(16,7))\n",
        "    sns.heatmap(precision, cmap = \"Blues\", annot = True, fmt = \".2f\", xticklabels=labels, yticklabels=labels)\n",
        "    plt.title(\"Precision Matrix\", fontsize = 30)\n",
        "    plt.xlabel('Predicted Class', fontsize = 20)\n",
        "    plt.ylabel('Original Class', fontsize = 20)\n",
        "    plt.tick_params(labelsize = 15)\n",
        "    plt.xticks(rotation = 90)\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"-\"*125)\n",
        "    \n",
        "    plt.figure(figsize=(16,7))\n",
        "    sns.heatmap(recall, cmap = \"Blues\", annot = True, fmt = \".2f\", xticklabels=labels, yticklabels=labels)\n",
        "    plt.title(\"Recall Matrix\", fontsize = 30)\n",
        "    plt.xlabel('Predicted Class', fontsize = 20)\n",
        "    plt.ylabel('Original Class', fontsize = 20)\n",
        "    plt.tick_params(labelsize = 15)\n",
        "    plt.xticks(rotation = 90)\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkKZ2nIzAueb",
        "colab_type": "text"
      },
      "source": [
        "### Test Data of Human Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJIHT2SaAuec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = load_model(\"../Data/Model_Save/model.h5\")\n",
        "predicted_labels = []\n",
        "true_labels = []\n",
        "batch_size = 10\n",
        "total_files = int(len(Test_Data) / batch_size) #here, I have added 2 because there are 30 files in Test_Humans\n",
        "for i in range(1, total_files, 1):\n",
        "    img_load = np.load(\"../Data/Bottleneck_Features/Bottleneck_TestData/bottleneck_{}.npy\".format(i))\n",
        "    img_label = np.load(\"../Data/Bottleneck_Features/TestData_Labels/bottleneck_labels_{}.npy\".format(i))\n",
        "    img_bundle = img_load.reshape(img_load.shape[0], img_load.shape[1]*img_load.shape[2]*img_load.shape[3])\n",
        "    for j in range(img_bundle.shape[0]):\n",
        "        img = img_bundle[j]\n",
        "        img = img.reshape(1, img_bundle.shape[1])\n",
        "        pred = model.predict(img)\n",
        "        predicted_labels.append(pred[0].argmax())\n",
        "        true_labels.append(img_label[j].argmax())\n",
        "acc = accuracy_score(true_labels, predicted_labels)\n",
        "print(\"Accuracy on Test Data = {}%\".format(np.round(float(acc*100), 2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_2nFdRaAueg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print_confusionMatrix(true_labels, predicted_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVAUWvzwAuer",
        "colab_type": "text"
      },
      "source": [
        "## 13. Testing on Real World with Still Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b65cwKscAues",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now for testing the model on real world images we have to follow all of the same steps which we have done on our training, CV\n",
        "# and test images. Like here we have to first pre-preocess our images then create its VGG-16 bottleneck features then pass those \n",
        "# bottleneck features through our own MLP model for prediction.\n",
        "# Steps are as follows:\n",
        "# 1. Read the image, convert it to grayscale and save it.\n",
        "# 2. Read that grayscale saved image, the detect face in it using HAAR cascade.\n",
        "# 3. Crop the image to the detected face and resize it to 350*350 and save the image.\n",
        "# 4. Read that processed cropped-resized image, then reshape it and normalize it.\n",
        "# 5. Then feed that image to VGG-16 and create bottleneck features of that image and then reshape it.\n",
        "# 6. Then use our own model for final prediction of expression."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7_p0O7cAuex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMOTION_DICT = {1:\"ANGRY\", 2:\"DISGUST\", 3:\"FEAR\", 4:\"HAPPY\", 5:\"NEUTRAL\", 6:\"SAD\", 7:\"SURPRISE\"}\n",
        "model_VGG = VGG16(weights='imagenet', include_top=False)\n",
        "model_top = load_model(\"../Data/Model_Save/model.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wCNqmn9Aue3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_prediction(path):\n",
        "    #converting image to gray scale and save it\n",
        "    img = cv2.imread(path)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    cv2.imwrite(path, gray)\n",
        "    \n",
        "    #detect face in image, crop it then resize it then save it\n",
        "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml') \n",
        "    img = cv2.imread(path)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "    for (x,y,w,h) in faces:\n",
        "        face_clip = img[y:y+h, x:x+w]\n",
        "        cv2.imwrite(path, cv2.resize(face_clip, (350, 350)))\n",
        "    \n",
        "    #read the processed image then make prediction and display the result\n",
        "    read_image = cv2.imread(path)\n",
        "    read_image = read_image.reshape(1, read_image.shape[0], read_image.shape[1], read_image.shape[2])\n",
        "    read_image_final = read_image/255.0  #normalizing the image\n",
        "    VGG_Pred = model_VGG.predict(read_image_final)  #creating bottleneck features of image using VGG-16.\n",
        "    VGG_Pred = VGG_Pred.reshape(1, VGG_Pred.shape[1]*VGG_Pred.shape[2]*VGG_Pred.shape[3])\n",
        "    top_pred = model_top.predict(VGG_Pred)  #making prediction from our own model.\n",
        "    emotion_label = top_pred[0].argmax() + 1\n",
        "    print(\"Predicted Expression Probabilities\")\n",
        "    print(\"ANGRY: {}\\nDISGUST: {}\\nFEAR: {}\\nHAPPY: {}\\nNEUTRAL: {}\\nSAD: {}\\nSURPRISE: {}\\n\\n\".format(top_pred[0][0], top_pred[0][1], top_pred[0][2], top_pred[0][3], top_pred[0][4], top_pred[0][5], top_pred[0][6]))\n",
        "    print(\"Dominant Probability = \"+str(EMOTION_DICT[emotion_label])+\": \"+str(max(top_pred[0])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1HL65pTAue7",
        "colab_type": "text"
      },
      "source": [
        "### ANGRY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eutAKvVAue8",
        "colab_type": "text"
      },
      "source": [
        "### Correct Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wddGv_yNAue-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Image.open(base_path+\"/validation/angry/7559.jpg\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkfDMYvjAufM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "make_prediction(base_path+\"/validation/angry/8151.jpg\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHVMJuYsAufT",
        "colab_type": "text"
      },
      "source": [
        "### Correct Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0wm5y0RAufU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Image.open(\"../Data/Test_Images/Angry_2.png\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olio5lH0Aufb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "make_prediction(\"../Data/Test_Images/Angry_2.png\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKlBg7a8Auff",
        "colab_type": "text"
      },
      "source": [
        "### DISGUST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpbZ95JEAuff",
        "colab_type": "text"
      },
      "source": [
        "### Incorrect Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XCkQXlOAufg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Image.open(\"../Data/Test_Images/Disgust_1.jpg\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOFRcg9kAufn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "make_prediction(\"../Data/Test_Images/Disgust_1.jpg\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lP-2wA8Aufv",
        "colab_type": "text"
      },
      "source": [
        "### Correct Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdatDNMBAufw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Image.open(\"../Data/Test_Images/Disgust_2.png\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFdLU-T0Auf6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "make_prediction(\"../Data/Test_Images/Disgust_2.png\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olS7tCFDAuf_",
        "colab_type": "text"
      },
      "source": [
        "### FEAR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cT_gUSNyAugA",
        "colab_type": "text"
      },
      "source": [
        "### Correct Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Shg_ebs-AugB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Image.open(\"../Data/Test_Images/Fear_1.jpg\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCj4BIjRAugG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "make_prediction(\"../Data/Test_Images/Fear_1.jpg\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnnQvmTXAugL",
        "colab_type": "text"
      },
      "source": [
        "### Correct Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qu_KXuUrAugM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Image.open(\"../Data/Test_Images/Fear_2.png\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2-RCUUEAugQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "make_prediction(\"../Data/Test_Images/Fear_2.png\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4dpwaaVAugV",
        "colab_type": "text"
      },
      "source": [
        "### HAPPY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98cSAgaxAugW",
        "colab_type": "text"
      },
      "source": [
        "### Correct Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_PiTjlxAugX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Image.open(\"../Data/Test_Images/Happy_1.jpg\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0g5ugthAugb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "make_prediction(\"../Data/Test_Images/Happy_1.jpg\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDW8qJY8Augg",
        "colab_type": "text"
      },
      "source": [
        "### Correct Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdWDVTIRAugh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Image.open(\"../Data/Test_Images/Happy_2.png\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjTW8yTJAugl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "make_prediction(\"../Data/Test_Images/Happy_2.png\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwanNW0JAugp",
        "colab_type": "text"
      },
      "source": [
        "### Neutral"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V69I60AAugq",
        "colab_type": "text"
      },
      "source": [
        "### Correct Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "632aceJ3Augr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Image.open(\"../Data/Test_Images/Neutral_1.jpg\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9h8h6lF6Augv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "make_prediction(\"../Data/Test_Images/Neutral_6.jpg\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJr9sgfIAugy",
        "colab_type": "text"
      },
      "source": [
        "### Sad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVs2xeodAugz",
        "colab_type": "text"
      },
      "source": [
        "### Correct Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUBBmGYkAug0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Image.open(\"../Data/Test_Images/Sad_1.jpg\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8XNdyzxAug4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "make_prediction(\"../Data/Test_Images/Sad_1.jpg\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQncn59wAuhC",
        "colab_type": "text"
      },
      "source": [
        "### Correct Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdxernhFAuhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Image.open(\"../Data/Test_Images/Sad_2.png\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPa6OR6PAuhI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "make_prediction(\"../Data/Test_Images/Sad_2.png\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJgatW28AuhO",
        "colab_type": "text"
      },
      "source": [
        "### Surprise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcLbBLQWAuhP",
        "colab_type": "text"
      },
      "source": [
        "### Correct Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7DALenPAuhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Image.open(\"../Data/Test_Images/Surprise_1.jpg\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kcb_AFsvAuhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "make_prediction(\"../Data/Test_Images/Surprise_1.jpg\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ULwCxwvAuhY",
        "colab_type": "text"
      },
      "source": [
        "### Correct Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PRjWIGWAuhY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Image.open(\"../Data/Test_Images/Surprise_2.png\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lxd-Sc0gAuhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "make_prediction(\"../Data/Test_Images/Surprise_2.png\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBeKdZ4eAuhh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Image.open(\"../Data/Test_Images/Surprise_3.jpg\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EWC-TKhAuh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "make_prediction(\"../Data/Test_Images/Surprise_3.jpg\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAH0YbOYAuh_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cnt_correct = 0\n",
        "# cnt_incorrect = 0\n",
        "# for i, d in df_anime_test.iterrows():\n",
        "#     img_path = os.path.join(d[\"folderName\"], d[\"imageName\"])\n",
        "#     im_size = cv2.imread(img_path).shape\n",
        "#     if im_size == (350, 350, 3):\n",
        "#         cnt_correct += 1\n",
        "#     else:\n",
        "#         cnt_incorrect += 1\n",
        "# print(\"Correct = \"+str(cnt_correct))\n",
        "# print(\"incorrect = \"+str(cnt_incorrect))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVflFj6KAuiC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# a = Train_Combined\n",
        "# randInt = np.random.randint(0, a.shape[0], size = (1))[0]\n",
        "# emotion = a[\"Emotion\"][randInt]\n",
        "# label = a[\"Labels\"][randInt]\n",
        "# path1 = a[\"folderName\"][randInt]\n",
        "# path2 = a[\"imageName\"][randInt]\n",
        "# img = Image.open(os.path.join(path1, path2))\n",
        "# img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp3VOJueAuiG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(emotion)\n",
        "# print(label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Qqf1IY6AuiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# count_present = 0\n",
        "# count_absent = 0\n",
        "# for i, d in df_angry_reduced.iterrows():\n",
        "#     path1 = d[\"folderName\"]\n",
        "#     path2 = d[\"imageName\"]\n",
        "#     if os.path.isfile(os.path.join(path1, path2)):\n",
        "#         count_present += 1\n",
        "#     else:\n",
        "#         count_absent += 1\n",
        "# print(\"Count present = \"+str(count_present))\n",
        "# print(\"Count absent = \"+str(count_absent))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1pOM4AAAuiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}